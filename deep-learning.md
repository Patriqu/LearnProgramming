# YouTube

[How Deep Neural Networks Work - Full Course for Begineers - freecodecamp](https://www.youtube.com/watch?v=dPWYUELwIdM)  


[Neural Networks]
CNNs (Convolutional Neural Networks)
- revolutionized the field of "Computer Vision"

RNNs (Recurrent Neural Networks)
- usage: Natural Language Processing (NLP)

[Machine learning Python libraries]
- Tensorflow
- Keras
- PyTorch
- numpy

[Neural Activation functions]
- sigmoid
- Softmax
- elu
- selu
- softplus
- softsign
- relu
- tanh (hyperbolic tangent activation function)
- hard sigmoid
- exponential
- linear

[Loss (cost) functions]	https://en.wikipedia.org/wiki/Loss_function
- squared error loss
- mean squared error (MSE)
- cross-entropy loss
- quadratic loss function
- 0-1 loss function

[Optimizers]
- Stochastic gradient descent (SGD)
- RMSprop
- Adagrad
- Adadelta
- Adam
- Adamax
- Nadam

[Recap of Neural Network build]
- introduced "neurons", the building blocks of "neural networks"
- Used the "sigmoid activation function" in our neurons
- Saw that neural networks are just neurons connected together
- Created a dataset with Weight and Height as inputs (or "features") and Gender as the output (or "label")
- Learned about "loss functions" and the "mean squared error (MSE)" loss
- Realized that training a network is just minimizing its loss
- used "backpropagation" to calculate "partial derivatives"
- used "stochastic gradient descent (SGD) to train our network


